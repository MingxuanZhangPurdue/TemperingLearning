{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 16\n",
    "train_batch_size = 32\n",
    "T = 100\n",
    "num_epochs = T + 1\n",
    "learning_rate = 5e-4\n",
    "lr_warmup_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAQABADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDprfVtT8L64bdpZJQrsJ4my/m8n5h6Hp+H1pt3qN74r12OB2kjilZRDCV2+WCR83ueOvua6PxVbj/hIrb92MzqqKQMFjnHXuab4Zg8nxNNBhWNurI2RkjHAPtXJaXNy30NtLXP/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAACnklEQVR4AVWSW2sTQRzF9zJ7md3ZW7K5N2lM2mBLEKsvhVKxItIXEXz0yW/gR/DZz+CbqCBFaZEWES8grYLF0oq3tJI2TdslJtlk083m0uyuIy1CDgzzMOfHn/85QxZ+fttYX/UJemb2WiI5StM0Maxut7u18Wn13YplVvJTM+SD+/c6dl0PabKqRZL5mSvzIpL+I7bder38zNj7mhtLfVzbAAykiIEjinx+MmsYB+trS69ePmo2GoTve57XMOsLTx42jM3P61u+NwioEhREwEIBQdpqtZy2HY0oi8+fOk735u27BEG9f/Ni7cNSKKBcnb3wp1JlGDYaz5Eri49LhS+e11dUpri7rypsUFcC8bmAHitsLmiS96NQ09TgoO+TJD87fwdcnp4z9rd51rOsWiIe9gbtWtX8vbt8bLsBqWOJrIQQRdKApgCvpNLjlB6KpjKTNA2aTWevVC8fmP0+CRmfJR1ZVjtdt1ismKYtiGJyNMMLAqAoaiJ/6e3edwlJFOkTBDtxPnPSa9XqbDabaFqe75U4lnWcXmQki9cAOMFgOB6OpU/6XchzsiJCyFEkhLDve6Qso9F0huUkLZQ4N57HZgofHorTc7dG0hOBYNAduISPu+MHAzwctu0OEgWI1KnpG0hSzgB8AUCf9GySIAFu2mc4XlEUHTMcC1uW3Tk2rUYN27D+TcCqVgxv0HM9V1FUnkfYrQXDgOEgRPiz4NbMqnHqPAMkJCIEcbuHR5Vy+YhhYXRkzLKOy2UDAIBfadIdAngBua4Xi0b0kE7RFMnAaGqSICkIeVVTeI4VkToEcBBxvLizU2zbjk9QSE3QAFI0TwOmVjMtq4VjGgJwwLmL1xlBL5UrHtBiyXFBlFEw/Wt799Aw5UhO06OnwF/uBQcorwHGfwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=16x16>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"huggan/smithsonian_butterflies_subset\", \n",
    "    split=\"train\", \n",
    "    cache_dir=\"./cache\"\n",
    ")\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
    "\n",
    "Image.fromarray(((sample_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=train_batch_size, \n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,126,275 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(32, 32, 64, 64, 128, 128),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"The model has {num_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(L.LightningModule):\n",
    "    def __init__(self, model, noise_scheduler):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[\"images\"]\n",
    "        noise = torch.randn_like(images)\n",
    "        steps = torch.randint(self.noise_scheduler.config.num_train_timesteps, (images.size(0),), device=self.device)\n",
    "        noisy_images = self.noise_scheduler.add_noise(images, noise, steps)\n",
    "        residual = self.model(noisy_images, steps).sample\n",
    "        loss = torch.nn.functional.mse_loss(residual, noise)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=lr_warmup_steps,\n",
    "            num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDUOp37Neare31y5hgLrApBDyHeCBGSQQB8uAM4zxzRZalKsmpPCqT6iYBNJbiQKYiCXABLBAowQ6YywC5LYNLdmRYI78W01rNLiRbeYvBJvyAUIO7G4BSck/6zGT1NKye4Im+wQCJYrl5FlABO4HhiwAO4+mTxjpmuKT5G41Fo7XXzT89vJ/eXKjTlJTaV11X3NX6p7P8ATpfuLs6Pa2trLfmxl1BbfZFZ25slilAJO/b8wDAKoTIGAdwzkmmhvbhtMulE1pqhHlOhYlkJYbCOwGV4wMYz9KikfUk0ppbiCOWKZB5gVQkYx03f3eOBjA4q1FPFFHNdyTNErPIqS2y7jHJJmON1dzgbSS2SQoz19W6sqstXp979e70XVt6bm1Kk7qO7f39F/l1+8doQtZdMZUu72e2ns42tLW4uUd0djukZ3UkvIWlAIxhV2bsEqK1bSNm8zy4hFESBGoiKA5A5IPJ4/kKoaTqklul3EsVtBYXm6W1zLtVUTznkcSbRvzg8MVb72QdjMbmmSxSaqBFb6gJY2y8W0uqgYzwuW7+ma1r0pyak18V2rdm9F8v60JjK8pK1mm1ZeX3fd8rgsbx2ESbd7I6K2VJzyMH8Ov4VQ1F44YrO6k0691G3a5AFrZMTKxCybyqqy9EKMAxYks2AOTUsmoadvhkXVJ9S2oqG7thGkc0ijDHaCApJzx6EYzwasTX7XDizt5FltoMXF5Aly24oJAWBKOGDoqA4YYIfjnArOUPZOUZJ2Ts9H3t2vrtbS90r6jvKSio9flutm0paPTpZ6K+p/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAALJklEQVR4AQ2W23NUh2HGz3XP/ew5e3a1OtJqL1qhO0hIICCSANkEZOi4qYsTu0nNUyczmelMm4c+ONPJpO1bU3eSNnbbmbidNm7Hk8SEYCfBBgyRISBAskASktBttdr72bOXc9tzr/6D73v4fb8PNI2mDAW07ApYI+Iix6DdtXA5UhcMTgVszK8oNEcVGduxWBIACUPC+eYW14apdnALs2K4q5s46poGQMARKbOyvHbvwaNPFzMbM6MT6ejQS9+6gkgu8fCXv1h4fMvz5KX1ynePvrXy6hfZWIbUgLDo1eII47O4kpFNEhW0lo56mtAMQHDE9TsszWqwYBggPdeVrv3+o3+5/jZHolLe1apavAP/+NEnHO6Acx+8f/fjd6f+6Nv/9+l7eDEZmRICMAa6zomx18698ZJf9WAkVHeyqBGGyQZmhlzKQuSDTlshps+l95mMsdYW+vid9zKF24cTowM8i7ehf/U3v7p4ZqRa0Y39Hejq53cMpsMoZHLPKhcupS+PjPae6WVo4t7qf/70nR/bHi5BVYbmIAaxvLAe0j2WRCCdxgIwmLO2jXKd/ehvv19bu2kpLuW4vhDcL0JdMbLUvB8/G1F7IcgKeBfGL5VZujPClUId5YF4XxiubBc31tflrfl3/v3vaqtF34ShlkwBvmNhOlKp0Q3XEe111mma//3eD0KwxA34t69uPHxU2Hu2HIex2ckTihQoffks2ZmCIiT8hf55ZX3j6MhMb7tgzT/99Oryr1ZW3KfaqbPHw7D5T//zvcX8nsuCStCiAIvKhPCauag8z6P17//bD5eXb6fGx4cjJyBQub70a2GKLdJ4trHTNZEMt9i+9gEI8/1wIBXu8AqozbNEfKaXl2rNp07Pm2Ngjbbc6me3lz748TsrqzstSUYVjRDkZ0EPWjCuf/i+8WXj1vLDn75/1ceJP798eTw5/uhOYXtvvjOK0AchWqWB4W4oPJjcyZVCLM8H8IfP56//8ydOd8+VP+661D7ymwef/eR7P+EDPousfnHjWiFnNwRCkcO2ur/eWthtPBifBTw8/r/3bkrzj0ZHg995+5uj4cTugxe4mzBVL9V/hEGPQOmhNyhXkLN2vIurVEEWKV+aHj35tdOr0tbC8+VEX89fvnl8eCq++/TG/bnrcrXW5DbdeTD7+e2TEexcd+K/3p39i9nT1140NAa89eGdbFMOMijO2VvZyhR3NNLOQi+PD5/qHfw8v+lb0UJuRWe6fvSPf79fKinNKskP/vU/zL4yOYjBxNi54eW1W49vbipZec5+BlkkSHNFVib2wi9NnxJBHGhYpZ3c+mePTo6NE559rOdEx2mxnsQRq2UuTG6PrT/Cvcq589ONgnPnkxuTV4IvmpUjaWJlqXTrfiEmaCKXOh5DlxZ/+bvfSJE2b/oENb++d2NObhslkuTIyHQyNH7kT9zgRv4RzMM35vIkHeI5kdcDoIIRTt35+c9+ZBrLRAvr7EtkVvKDR0iS6ZpbvC+ScCycv/uglq9YTHy4ls3tl2qHSA4RWrYLDE6ISaLnZ6sPv0KlEUJ/klVd2bh08tgHn/3+tVcv/+lb34ZoG6INjw43XznzlYXdCm47T5+srhn5H/7gw8r2/JNbG+/+662HuzSd4E8Ni6KghTqwAE6TaDKNjAz29wutwJcba6W57OJyph1IFdfKXz1/NkZ3ff3ijIUwUIv2FB5iUBk2EILre60vleofOPKN45d4ShS7HIivlbdnrkzSJrLxGMjnuhkvxBNoImWlj8Kdw61iq3rjwyVgn7508ZxloKDgTA30IDD9vPjMd+WTw+M+X2/hIGL6hhngw1zufjK+VqhbHxkmgR+fnihJ8sToSEPbfL7bK/aL03EaDsI3shGhjnqU2ccnursjm3Tf8rNlr6bNHudpgq95m1V5U6pkT8cu06NdFB6pNxVoS3XcahFwSfbQWD2z6RDAkwdPhTBWVy2V3tpbwt741tA3jo9rWKS8C461Uyir9jCxbdwo1JDJsaHzV16jMO/awtanV3+rNWCb6RjoncxHpGGNVlwL4qNIH8ZoTBulSvGE2DYyTFnw6+LU6v1loIbjJ44edjK4KpQ9RQ2DXYily6DgJnxO9mUUpoK5TEaLUVqGeP31bzpGVbK1LpjMqvW3orMFyiQyNAS2EIPpRF1dbQaUcF/6xs0/PLlLcLl2PE5/nWcQB+qZhpDirlzmdukKRXBhUPqyFZRgIYoi9SaP9dYlJX44kiCgbIHu6RiQi2szsy/jkycF2teSFcXXEMjcNFycwmDHjUizJ/vXDs0p8882P5lBEqDY2Xq69EWhlD6YLDvkYQYsg43wjhagIiAL8ZGyru3n6vGelGdibel2zjMmvvu268tQ2bOghkuFg56JqJXWAZNaEMCwKlJpgASTXul0Xx7NqLL5uLCzWI5OCYTK1zsQS/aR/YaoAo4RcWwWwpAceKCH+vKmwgSIQLOwKmgXSm8m2EOAXyvSkai+qgIi5IY6IEh1zZreYFiQ4pry+trdvbvrfQGSdr3OVJgnvCpmCu0NHuKA/jgpxFKYqDsWoJg9EBEkE2BlVww1uydSCRquvdj1oqqFiLG6ovBUw3ShNohwMYuoodGGYnNUuR7yLVToDBpBSkNJw28yrQQOSrU5ogLawXYf5foIs8JUOxACaIBeAtQNvY0TD8m7jZv3Slolh5fqgOWDHEEYkRBLQibaMpodKipkYxRquJXyfGFd7+UG3/+Pa4GWVXUtzMvFk4fZzsBhoE3K97OMoUKk7ZakGgqjSMYyJfJFpbhZM4tDkzGUUvKQ56WyVVtTWNRHCQgEVTZkYrzF1gyCIVb1NU+U4T7u1bMXB84cnhiaYoX+miEEq2wpIkVDLkkGW6GQHeYpIJoMcnEj+MboBVKIVCSXcaLz1+7rVoteQyEIa0qQI5fAkI9boK05WsCIFtGStL0ANNX18n6jmkcCGEd6Vs7gg5iEA21MAEwcwzM1EzOLWzsu7KfYnoWtO+PpUzXAg+Eg1766v+wmz73ZFRMDPhxyOgwwh5RgNVqn61IkKFRc3w9oXVvlj9xcvqQSJJDfrwINXOLzCT7FHCgv6gFe3wTv5Yqqsffreajb3iyu8tF+WS8HJW9z0x05lo6SYUFlEJ9xU1VF0Q9uRTMfVjHCsjknlLf4MB/xuEj00MWzg8PhBGbq3aF2PihVmzt8iwIR9PBAZ9oMG+UdqgNa06qHBnp3ntzrCXemzvSHh9tLu6oDNKuRcpYsOfseqVtI1u2LFm23U2+gqCX6iG4sWUVJf57MDCf7x4/GQ1aDEYXymlLN+0afyHimWmLVPZ7Sosmyb3YIqDGkxQzaZUONxT9Mn/pOKnDAmEgxsOVULTcO4USO6azQisa2argikV0oaAun4ENFSavn10Cb6EkDtQiTItoNzEmRbq2F6S3NxXkHcnoTKQRDx8CubbW+vHGng+3el8oBIlE0/SpgODDlahrilRirEzAQmJUt/UBAurM/PDr3i6sIUfvd08XWnhnu5+CCHhKF8a/9mavHLEoOpsU7c8bq4nUL6kahPEkm9EZ2amIIwGEkHnOiSLroF/OaFuuNIyrCRfWAy+DAegttE4Bmyyaadjw2c/rxvYXuYW0/hCZZYhMz2KOnR18+AzoOoQaatH90b7SwN0/qXguL8MEwjvFg3a2m214aShs1CHUAMaTVZUkJ1SHNo5uoXgXa0WpTkQgI6wJ97quz0xOXLkBIEqg5zSYYConHJs77im3pECnWWRRIxF8ZHDtvwBG7urNW2CtoMhIfPXtxBoJbKlhssna10Y6gmuVA/w9sVc4O/sjUgAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=200)\n",
    "\n",
    "noise = torch.randn(sample_image.shape)\n",
    "\n",
    "timesteps = torch.LongTensor([10])\n",
    "\n",
    "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlmodel = DiffusionModel(model, noise_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\mingx\\anaconda3\\envs\\mingxuan\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model | UNet2DModel | 7.1 M  | train\n",
      "----------------------------------------------\n",
      "7.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.1 M     Total params\n",
      "28.505    Total estimated model params size (MB)\n",
      "363       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\mingx\\anaconda3\\envs\\mingxuan\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\mingx\\anaconda3\\envs\\mingxuan\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4fcda2ba9d4d8d80574ae22edd49f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mingx\\anaconda3\\envs\\mingxuan\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:2358: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n",
      "`Trainer.fit` stopped: `max_epochs=201` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=num_epochs, precision=\"32\")\n",
    "trainer.fit(tlmodel, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 16\n",
    "seed = 42\n",
    "output_dir = \"./output\"\n",
    "\n",
    "pipeline = DDPMPipeline(unet=tlmodel.model, scheduler=noise_scheduler)\n",
    "    \n",
    "def evaluate(epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=eval_batch_size,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed), # Use a separate torch generator to avoid rewinding the random state of the main training loop\n",
    "        num_inference_steps=1\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6974f5d0024faea146247809d5ab96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(epoch=2, pipeline=pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mingxuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
