{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from lightning.fabric.utilities import AttributeDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from TL.generation import Tempering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images: torch.Tensor, filename: str):\n",
    "    images = (images / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "    grid = torchvision.utils.make_grid(images, nrow=5)\n",
    "    grid_np = (grid.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "    grid_pil = Image.fromarray(grid_np)\n",
    "    grid_pil.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arguments:\n",
    "\n",
    "    seed: int = 42\n",
    "    num_nodes: int = 1\n",
    "    strategy: str = \"auto\"\n",
    "    precision: str = None\n",
    "\n",
    "    image_size: int = 32\n",
    "\n",
    "    train_batch_size: int = 16\n",
    "    learning_rate: float = 1e-4\n",
    "    lr_warmup_steps: int = 500\n",
    "\n",
    "    T: int = 50\n",
    "    num_epochs_per_timestep: int = 2\n",
    "    burn_in_fraction: float = 0.8\n",
    "    zeta: float = 4.0\n",
    "    m: float = 0.5\n",
    "    random_walk_step_size: float = 1e-6\n",
    "    sample_prediction: bool = True\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "L.seed_everything(args.seed)\n",
    "\n",
    "# set fabric\n",
    "fabric = L.Fabric(\n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\",\n",
    "    num_nodes=args.num_nodes,\n",
    "    strategy=args.strategy,\n",
    "    precision=args.precision,\n",
    ")\n",
    "\n",
    "# launch fabric\n",
    "fabric.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 63\n"
     ]
    }
   ],
   "source": [
    "# set dataset\n",
    "train_dataset = load_dataset(\n",
    "    \"huggan/smithsonian_butterflies_subset\", \n",
    "    split=\"train\", \n",
    "    cache_dir=\"./cache\"\n",
    ")\n",
    "\n",
    "# set transform\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((args.image_size, args.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "train_dataset.set_transform(transform)\n",
    "num_training_samples = len(train_dataset)\n",
    "\n",
    "# set dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=args.train_batch_size, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "num_training_batches = len(train_dataloader)\n",
    "\n",
    "print (\"Number of training batches:\", num_training_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 7126275\n"
     ]
    }
   ],
   "source": [
    "# set model\n",
    "model = UNet2DModel(\n",
    "    sample_size=args.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(32, 32, 64, 64, 128, 128),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training timesteps: 1000\n"
     ]
    }
   ],
   "source": [
    "# set noise scheduler\n",
    "noise_scheduler = DDPMScheduler()\n",
    "print (\"Number of training timesteps:\", noise_scheduler.config.num_train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MC steps: 126\n",
      "Burn in steps: 100\n",
      "Number of MC samples: 26\n",
      "MC subset size: 13\n",
      "Number of training samples: 1000\n",
      "Zeta: 10.0\n",
      "Random walk step size: 1e-06\n",
      "timestep mapping: tensor([  0,  20,  40,  61,  81, 101, 122, 142, 163, 183, 203, 224, 244, 265,\n",
      "        285, 305, 326, 346, 366, 387, 407, 428, 448, 468, 489, 509, 530, 550,\n",
      "        570, 591, 611, 632, 652, 672, 693, 713, 733, 754, 774, 795, 815, 835,\n",
      "        856, 876, 897, 917, 937, 958, 978, 999])\n"
     ]
    }
   ],
   "source": [
    "# set tempering learning\n",
    "num_mc_steps = num_training_batches * args.num_epochs_per_timestep\n",
    "tlmodel = Tempering(\n",
    "    model=model,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    T=args.T,\n",
    "    num_mc_steps=num_mc_steps,\n",
    "    num_training_samples=num_training_samples,\n",
    "    zeta=args.zeta,\n",
    "    mc_subset_ratio=args.m,\n",
    "    burn_in_fraction=args.burn_in_fraction,\n",
    "    random_walk_step_size=args.random_walk_step_size,\n",
    "    sample_prediction=args.sample_prediction,\n",
    ")\n",
    "\n",
    "print (\"Number of MC steps:\", tlmodel.num_mc_steps)\n",
    "print (\"Burn in steps:\", tlmodel.burn_in_steps)\n",
    "print (\"Number of MC samples:\", tlmodel.num_mc_samples)\n",
    "print (\"MC subset size:\", tlmodel.mc_subset_size)\n",
    "\n",
    "print (\"Number of training samples:\", num_training_samples)\n",
    "print (\"Zeta:\", args.zeta)\n",
    "print (\"Random walk step size:\", args.random_walk_step_size)\n",
    "\n",
    "print (\"timestep mapping:\", tlmodel.timestep_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate\n",
    ")\n",
    "\n",
    "# set lr scheduler\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps,\n",
    "    num_training_steps=args.T*args.num_epochs_per_timestep*num_training_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up objects\n",
    "tlmodel, optimizer = fabric.setup(tlmodel, optimizer)\n",
    "train_dataloader = fabric.setup_dataloaders(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training timesteps:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\mingx\\anaconda3\\envs\\mingxuan\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:2358: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n",
      "Training timesteps:  94%|█████████▍| 47/50 [31:22<02:00, 40.04s/it, Avg train loss=0.7332]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m avg_train_loss_per_timestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     24\u001b[0m fabric\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtlmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_before_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\mingx\\OneDrive\\Desktop\\VSCodeGithub\\TemperingLearning\\TL\\generation.py:195\u001b[0m, in \u001b[0;36mTempering.on_before_optimizer_step\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m    193\u001b[0m denominator \u001b[38;5;241m=\u001b[39m exp_norm_squared\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Update the gradient of the parameter\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzeta\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m weighted_diff \u001b[38;5;241m/\u001b[39m (denominator \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_training_samples)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "tlmodel.train()\n",
    "\n",
    "progress_bar = tqdm(range(args.T), desc=\"Training timesteps\")\n",
    "\n",
    "for t in progress_bar:\n",
    "\n",
    "    tlmodel.reset_sample_buffers()\n",
    "\n",
    "    batch_idx = 0\n",
    "\n",
    "    avg_train_loss_per_timestep = 0\n",
    "\n",
    "    for epoch in range(args.num_epochs_per_timestep):\n",
    "\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = tlmodel.training_step(batch, t)\n",
    "\n",
    "            avg_train_loss_per_timestep += loss.item()\n",
    "\n",
    "            fabric.backward(loss)\n",
    "\n",
    "            tlmodel.on_before_optimizer_step(t)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            tlmodel.on_train_batch_end(lr_scheduler.get_last_lr()[0], batch_idx)\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "    avg_train_loss_per_timestep /= (args.num_epochs_per_timestep * num_training_batches)\n",
    "    progress_bar.set_postfix({'Avg train loss': f'{avg_train_loss_per_timestep:.4f}'})\n",
    "\n",
    "    tlmodel.eval()\n",
    "    images = tlmodel.generate_samples(args.train_batch_size, t)\n",
    "    save_images(images, f\"generated_samples/images_{t}.png\")\n",
    "    tlmodel.train()\n",
    "\n",
    "state = AttributeDict(model=model)\n",
    "fabric.save(\"output/model.pt\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCzJe/YLy606cBmjQLpqXOEW2EJfYCrABXeIBwMqjMJYzjDJWq2u2KaxqEUE2lzw2dt58sVmVcSqUKsu5yRvRlhG7K4UrwQpzQsJbvZc2CkNetLG1xFHcRwRiZ5F3YaQghfmiRWG5tk0ahSynEltZ/2ZfT3VtDGuqzqHllt5yqMpBBnMCk+YoyE2l23B9y7GY118lDk9lUSc4x6X6Jaq62V3dN2u732Pl6kniKjqpWUlZbcvbXS17WfxLWzdnqMvrfUG8Ny6Y9zfNb3DvE8QsY9kjowdpDIuw5KKYyoYM5clS+N7SXlmbm4sIL6BLTREv7h5vs0RhhuAjlUZyyAIJBKY2QsVYspXALkMsNJvdIvNNs5LdZJTNu+0CF50s4Yi4VHJw2VLDY7E4BBEasoByo9Du7qCbWdGnnjhhhwW0hvKUlBhmifcIzHl5SPukgdRJ5ueyolBRUtFd29297p2e+6b0elrNappm+HknUspK2q0bfxPltazV1r3Wlk0rJ73ifWGmuH+3Q2k2kQkNMr2YlJtyUkLrkYXbhyy5YkogzllpNPh1GGKy8+H7ZbRXplBiu5srcRLIxjkDoEXa6t0wpkKYRFJUPtpdP1Ce2ZtNj0hliRrazaEglVupNvmFAAM7XbYpAAbgMKu2pYNc/YwLOUsZLqJmO8rLuCuknVMjG1i5AWPChSCB59OuqVoNWa8nbtdWvpZWsrrTfa3kVJfVpyk221rrq0r9Ne7bTTX3WtgmxSK1udQjtbmOO08mW2hvC0cOxZMqnyv90li4EgIjK4KIAd1fw/NpGl3c1mdVns1vv3jzm8ZsoBGVkUyody75ZyPlyzsOxbZauhPb6xca3HJdu1osgmclYo5ix++JBgghYyivKAM7GUuyjdctLJr5GuNW02GznsUtvNlWCVIlIRdghYEKVGXbHCpu2BWBYnriqc6TVRPleqt568vm9dmrK+12jpoQnOlGTV9Nu6SXTrryrS+rfxO9v/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAFS0lEQVR4AVVWC4hVVRTdNpaaZUZq9gEzyDArx8DScGTIIAI1S6ihj4KlZpSJqSAIGqlhA1pjqIUVJKlNmKmISWr5I5wKE3+U9rGisMJUGtQCV2utfe5I9527zz5rr7X2ee/dd++LiAA4luU0TWuPQwJUzCVwDaJG+anhZCBOs2Kl+BYzXsfKdGkKxortRPxLsiyhtShNZBjpgsGVU92zAJ7kEttU3ZUOy4HLhMkwNlXdn8g1jnxMdNF5SXcD51GP6IjYGjcTkCDfo3dYLOIe4cCHpR3weCx1A8LEMbQ2vZW3EtqLGJ9tVddrTU5uEeiBTna80mpzXI/YAfz8SroKwR9uTEa9BIIaq0S+dsFYUx8Zru0FBo4V8wfga7zdggNEECcVeWjTEfOVoGmeQH9uuFROqouxTjOT9W8WTOwzBrsJj2hioR/ByhTrmXWsDIDbEf+sJa8NuSH2WbhQeo45rLDqD1Q0n8Q2qpjdPa3mMrostpHSlBeCGDVuofqdaRJXJel08IsYEs/aj8oBTtw1usoAsx2BFQLVVUKPDOIwMxTfNppxhZDXVf8zsIGlftZG8NOIcakjZbu2ulJLJnExk7dsxCRbKbHzBzLUuYhnD0PiNN4oOF+ceLExdpQm+GM4t38rRij/zZtjCc/EUbr0TjOb76Skk7yyVbysRFSdmibwG86U8Va8WqosfmkVBs/sDvSXTPQTCVo9GmghPFFyetagSbvhwfUM9GXUCAxCM5aIIQMdnL/yXMiaTmXFNBHHiGyv3ioXtdOuLLVVpyofo7qx611T4HmbQsROjGJtSwxG855AZ2KiRkwCpuyfSM5yi4uFNIj2vCtkxpZD1X8E6/GemCKiIeelOCk3aRRUG4ZYrSUPU1sswTtcKVP8iTdB1+4t0qTLIuqxI7BbXh5W2f9g1cCaUl0c0dN9N8fdNFkrh8gLwA3UDTjmKVBrqjZGFla5wZM2T7uM2ozeaG72oz2mCdRtzVqVvP5Ea+UMnGyrCm91joXHUh/EOVNZngZMtuqSFGp/vN1IwWqzo1wE5OSEuUb0j0FKBiifhHjQtAvttQ8deEOEtsPadCCc1oE5wR+N1rrp2l2lZVOUi+NJuAfXPIS3FakULT4VN2JFRIsoYl0YPQ0ZnE68JuJd/gL6Ykah/VKRSQmsk7st5PBrpl0SVF8dEpbOIs3Tmo9cHt3jciHSZ9dVMrAg/mZ9WOaSjxc6XyyXGW+SRdQxVAjixzjjTkSSivhehONuJ1Tsx3I3dWqMZX4EMz3B6/klVnE2RqUJYnLqEHNnS6jtdHZtJOK4myAOyqUa9tcy8IBAvl60R0Rdq/DPjaWkbPFo4ClbWyDSlgZFDCn75WWz0Sb+KzJYpTLGOgGu1gXGpr3U2NUFwbfHG3k7Ic2qCfbJcIvYfjF8kRVFjxJ4i7ZK2llW8iNIj2RWljG9EspStWSrfgegB0PEaHQF+FG5zkevqCN1o0m+NLw9alFCmxcepjxwRKZ5KBF0AakFdhm6yA4kcLRj7MCE1AO2qJq5SXKwwQvxi5snN/8MuK9OBd4/22/D72quC+n/XtInuFCPOHJORnTYLhrxwwLMUHheaPgrcVU3V7x/SDmTw89lojwRx8BcLMF3A/BCPISBrsS1tKHxLPvxBqgWzrW3fcf4y1p1wKBp3kp0w7/oZd5d4ub+EI9KxyHmGsFaesbTSqQ4KwB8WFTl1NxPtKniM2/epDpHOmAzMC6ZFcLCa4XAa5JoTXmq8N9GcviBqyNipr2+UYw+1gB7rQl+ExOEpKY009pnQWXC/7A8eB1gAc8G1f8Dcqy5djxQX0UAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tlmodel.generate_samples(1)\n",
    "image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mingxuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
