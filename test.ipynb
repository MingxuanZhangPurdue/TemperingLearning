{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from lightning.fabric.utilities import AttributeDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from TL.generation import Tempering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images: torch.Tensor, filename: str):\n",
    "    images = (images / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "    grid = torchvision.utils.make_grid(images, nrow=5)\n",
    "    grid_np = (grid.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "    grid_pil = Image.fromarray(grid_np)\n",
    "    grid_pil.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arguments:\n",
    "\n",
    "    seed: int = 42\n",
    "    num_nodes: int = 1\n",
    "    strategy: str = \"auto\"\n",
    "    precision: str = None\n",
    "\n",
    "    image_size: int = 32\n",
    "\n",
    "    train_batch_size: int = 16\n",
    "    learning_rate: float = 1e-4\n",
    "    lr_warmup_steps: int = 500\n",
    "\n",
    "    T: int = 50\n",
    "    num_epochs_per_timestep: int = 2\n",
    "    burn_in_fraction: float = 0.8\n",
    "    zeta: float = 4.0\n",
    "    m: float = 0.5\n",
    "    random_walk_step_size: float = 1e-6\n",
    "    sample_prediction: bool = False\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "L.seed_everything(args.seed)\n",
    "\n",
    "# set fabric\n",
    "fabric = L.Fabric(\n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\",\n",
    "    num_nodes=args.num_nodes,\n",
    "    strategy=args.strategy,\n",
    "    precision=args.precision,\n",
    ")\n",
    "\n",
    "# launch fabric\n",
    "fabric.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset\n",
    "train_dataset = load_dataset(\n",
    "    \"huggan/smithsonian_butterflies_subset\", \n",
    "    split=\"train\", \n",
    "    cache_dir=\"./cache\"\n",
    ")\n",
    "\n",
    "# set transform\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((args.image_size, args.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "train_dataset.set_transform(transform)\n",
    "num_training_samples = len(train_dataset)\n",
    "\n",
    "# set dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=args.train_batch_size, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "num_training_batches = len(train_dataloader)\n",
    "\n",
    "print (\"Number of training batches:\", num_training_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "model = UNet2DModel(\n",
    "    sample_size=args.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(32, 32, 64, 64, 128, 128),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set noise scheduler\n",
    "noise_scheduler = DDPMScheduler()\n",
    "print (\"Number of training timesteps:\", noise_scheduler.config.num_train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tempering learning\n",
    "num_mc_steps = num_training_batches * args.num_epochs_per_timestep\n",
    "tlmodel = Tempering(\n",
    "    model=model,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    T=args.T,\n",
    "    num_mc_steps=num_mc_steps,\n",
    "    num_training_samples=num_training_samples,\n",
    "    zeta=args.zeta,\n",
    "    mc_subset_ratio=args.m,\n",
    "    burn_in_fraction=args.burn_in_fraction,\n",
    "    random_walk_step_size=args.random_walk_step_size,\n",
    "    sample_prediction=args.sample_prediction,\n",
    ")\n",
    "\n",
    "print (\"Number of MC steps:\", tlmodel.num_mc_steps)\n",
    "print (\"Burn in steps:\", tlmodel.burn_in_steps)\n",
    "print (\"Number of MC samples:\", tlmodel.num_mc_samples)\n",
    "print (\"MC subset size:\", tlmodel.mc_subset_size)\n",
    "\n",
    "print (\"Number of training samples:\", num_training_samples)\n",
    "print (\"Zeta:\", args.zeta)\n",
    "print (\"Random walk step size:\", args.random_walk_step_size)\n",
    "\n",
    "print (\"timestep mapping:\", tlmodel.timestep_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate\n",
    ")\n",
    "\n",
    "# set lr scheduler\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps,\n",
    "    num_training_steps=args.T*args.num_epochs_per_timestep*num_training_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up objects\n",
    "tlmodel, optimizer = fabric.setup(tlmodel, optimizer)\n",
    "train_dataloader = fabric.setup_dataloaders(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tlmodel.train()\n",
    "\n",
    "progress_bar = tqdm(range(args.T), desc=\"Training timesteps\")\n",
    "\n",
    "for t in progress_bar:\n",
    "\n",
    "    tlmodel.reset_sample_buffers()\n",
    "\n",
    "    batch_idx = 0\n",
    "\n",
    "    avg_train_loss_per_timestep = 0\n",
    "\n",
    "    for epoch in range(args.num_epochs_per_timestep):\n",
    "\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = tlmodel.training_step(batch, t)\n",
    "\n",
    "            avg_train_loss_per_timestep += loss.item()\n",
    "\n",
    "            fabric.backward(loss)\n",
    "\n",
    "            tlmodel.on_before_optimizer_step(t)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            tlmodel.on_train_batch_end(lr_scheduler.get_last_lr()[0], batch_idx)\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "    avg_train_loss_per_timestep /= (args.num_epochs_per_timestep * num_training_batches)\n",
    "    progress_bar.set_postfix({'Avg train loss': f'{avg_train_loss_per_timestep:.4f}'})\n",
    "\n",
    "    tlmodel.eval()\n",
    "    images = tlmodel.generate_samples(args.train_batch_size, t)\n",
    "    save_images(images, f\"generated_samples/images_{t}.png\")\n",
    "    tlmodel.train()\n",
    "\n",
    "state = AttributeDict(model=model)\n",
    "fabric.save(\"output/model.pt\", state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mingxuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
